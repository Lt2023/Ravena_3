{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3049ca18-e56f-459d-ae11-88a750225bd9",
   "metadata": {},
   "source": [
    "# 1.ä¿®æ”¹train_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259956d-fab1-4394-b4bd-04ccfb7c7511",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ä¸‹åˆ—åªå±•ç¤ºæ ¼å¼ç¤ºä¾‹,æ–‡ä»¶åœ¨ä¸»ç›®å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a6aa8-47af-4f47-9c1f-ad25ccf25f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"question\": \"ä½ æ˜¯è°ï¼Ÿ\",\n",
    "      \"answer\": \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚\"\n",
    "    },\n",
    "    {\n",
    "      \"question\": \"ä»Šå¤©çš„å¤©æ°”å¦‚ä½•ï¼Ÿ\",\n",
    "      \"answer\": \"ä»Šå¤©å¤©æ°”æ™´ï¼Œé€‚åˆå‡ºé—¨ã€‚\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad784ead-db5d-406a-aeba-4ac65ecb1241",
   "metadata": {},
   "source": [
    "# 2.è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2cf2e-0171-455c-b1e2-24e830f8eb90",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥åœ¨ä¸»ç›®å½•ä¸‹æ‰¾åˆ°train.pyæˆ–è€…è¿è¡Œä¸‹åˆ—ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ddfcf-6bfa-4a1a-88ab-1e732a482886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LanguageModel\n",
    "\n",
    "def main():\n",
    "    model = LanguageModel()  \n",
    "    print(\"å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "    model.train(epochs=1000)  \n",
    "    print(\"è®­ç»ƒå®Œæˆï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3fc93-527f-4a85-b322-60c5a6b4f5d1",
   "metadata": {},
   "source": [
    "# 3.æŸ¥çœ‹tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307c56dc-7ce4-4281-af4d-930a46f429f4",
   "metadata": {},
   "source": [
    "æœ¬é¡¹ç›®æä¾›äº†æ–‡ä»¶å®ç°Tokenizerå¯è¯»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c5045-9ba7-4509-b0db-0673ea573d9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from termcolor import colored\n",
    "\n",
    "def display_tokenizer(tokenizer_file='model/tokenizer.json', output_file='Ravena_tokenizer_output.txt'):\n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(tokenizer_file, 'r', encoding='utf-8') as file:\n",
    "            tokenizer_data = json.load(file)\n",
    "        print(colored(f\"æˆåŠŸåŠ è½½Tokenizeræ–‡ä»¶: {tokenizer_file}\", \"green\"))\n",
    "    except FileNotFoundError:\n",
    "        print(colored(f\"æœªæ‰¾åˆ°Tokenizeræ–‡ä»¶: {tokenizer_file}\", \"red\"))\n",
    "        return\n",
    "    \n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "    tokenizer = tokenizer_from_json(tokenizer_data)\n",
    "    \n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "\n",
    "    try:\n",
    "        with open(output_file, 'w', encoding='utf-8') as output:\n",
    "            output.write(\"Tokenizer è¯æ±‡è¡¨ï¼š\\n\")\n",
    "            for word, index in word_index.items():\n",
    "                output.write(f\"{word}: {index}\\n\")\n",
    "        \n",
    "        print(colored(f\"è¯æ±‡è¡¨å·²æˆåŠŸä¿å­˜åˆ°: {output_file}\", \"green\"))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(colored(f\"ä¿å­˜è¯æ±‡è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\", \"red\"))\n",
    "        return\n",
    "    \n",
    "    print(colored(\"Tokenizer è¯æ±‡è¡¨ï¼ˆéƒ¨åˆ†ï¼‰:\", \"yellow\"))\n",
    "    for i, (word, index) in enumerate(word_index.items()):\n",
    "        print(f\"{word}: {index}\")\n",
    "        if i >= 5000:  \n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_tokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59350f8-3f5d-443a-b519-7232ab6d83e5",
   "metadata": {},
   "source": [
    "# 4.å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed59bf6-2b71-4536-9c19-a9275ea199a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "æ‚¨éœ€è¦ä¿®æ”¹new_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8338afd-9870-4ed4-bc8e-ac3863ab910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "import random\n",
    "import pickle\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.layers import Layer\n",
    "import re\n",
    "\n",
    "\n",
    "# è‡ªå®šä¹‰çš„ PositionalEncoding å±‚\n",
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, max_len, model_dim, trainable=True, **kwargs):\n",
    "        super(PositionalEncoding, self).__init__(trainable=trainable, **kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # è®¡ç®—ä½ç½®ç¼–ç \n",
    "        position = np.arange(self.max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, self.model_dim, 2) * -(np.log(10000.0) / self.model_dim))\n",
    "        pos_encoding = np.zeros((self.max_len, self.model_dim))\n",
    "        pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
    "        \n",
    "        # å°†é¢„è®¡ç®—çš„ pos_encoding ä½œä¸ºæƒé‡ï¼Œä¸ä½¿ç”¨ Keras åˆå§‹åŒ–å™¨\n",
    "        self.pos_encoding = tf.Variable(initial_value=pos_encoding, trainable=False, dtype=tf.float32, name='pos_encoding')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'max_len': self.max_len,\n",
    "            'model_dim': self.model_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class FineTuningTool:\n",
    "    def __init__(self, model_file='model/.Ravena-LLM_Model.keras', tokenizer_file='model/tokenizer.json', faq_file='model/.faq_data.pkl', max_seq_length=20, vocab_size=10000):\n",
    "        # å°†æ¨¡å‹ä¿å­˜è·¯å¾„è®¾ç½®ä¸º models æ–‡ä»¶å¤¹ï¼Œå¹¶ä½¿ç”¨ .keras åç¼€\n",
    "        self.model_file = model_file\n",
    "        self.tokenizer_file = tokenizer_file\n",
    "        self.faq_file = faq_file\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # åŠ è½½æ¨¡å‹å’ŒTokenizer\n",
    "        self.model = self._load_model()\n",
    "        self.tokenizer = self._load_tokenizer()\n",
    "\n",
    "        # åˆå§‹åŒ–FAQæ•°æ®\n",
    "        self.faq_data = self._load_faq_data()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None\"\"\"\n",
    "        if os.path.exists(self.model_file):\n",
    "            return load_model(self.model_file, custom_objects={'PositionalEncoding': PositionalEncoding})\n",
    "        else:\n",
    "            print(f\"æ¨¡å‹æ–‡ä»¶ {self.model_file} ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå¾®è°ƒã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚\")\n",
    "            return None\n",
    "\n",
    "    def _load_tokenizer(self):\n",
    "        \"\"\"åŠ è½½Tokenizerï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None\"\"\"\n",
    "        if os.path.exists(self.tokenizer_file):\n",
    "            with open(self.tokenizer_file, 'r', encoding='utf-8') as f:\n",
    "                tokenizer_json = json.load(f)\n",
    "                return tokenizer_from_json(tokenizer_json)\n",
    "        else:\n",
    "            print(f\"Tokenizeræ–‡ä»¶ {self.tokenizer_file} ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå¾®è°ƒã€‚è¯·å…ˆè®­ç»ƒæ¨¡å‹ã€‚\")\n",
    "            return None\n",
    "\n",
    "    def _load_faq_data(self):\n",
    "        \"\"\"åŠ è½½FAQæ•°æ®ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ä¸ºç©ºå­—å…¸\"\"\"\n",
    "        if os.path.exists(self.faq_file):\n",
    "            with open(self.faq_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            return {}\n",
    "\n",
    "    def _save_faq_data(self):\n",
    "        \"\"\"ä¿å­˜FAQæ•°æ®\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.faq_file), exist_ok=True)\n",
    "        with open(self.faq_file, 'wb') as f:\n",
    "            pickle.dump(self.faq_data, f)\n",
    "\n",
    "    def augment_data(self, data):\n",
    "        \"\"\"æ•°æ®å¢å¼ºï¼šæ‰“ä¹±è¯åºå’Œéšæœºæ’å…¥/åˆ é™¤è¯è¯­\"\"\"\n",
    "        augmented_data = []\n",
    "        for item in data:\n",
    "            words = item.split()\n",
    "            random.shuffle(words)  # æ‰“ä¹±é¡ºåº\n",
    "\n",
    "            if random.random() > 0.5:\n",
    "                words.append(random.choice(words))  # éšæœºæ’å…¥è¯è¯­\n",
    "            if len(words) > 2 and random.random() > 0.5:\n",
    "                words.remove(random.choice(words))  # éšæœºç§»é™¤è¯è¯­\n",
    "\n",
    "            augmented_data.append(' '.join(words))\n",
    "        return augmented_data\n",
    "\n",
    "    def prepare_data(self, questions, answers):\n",
    "        \"\"\"å¤„ç†é—®é¢˜å’Œç­”æ¡ˆï¼Œè¿›è¡Œåˆ†è¯å¹¶ç”Ÿæˆåºåˆ—\"\"\"\n",
    "        # æ•°æ®å¢å¼º\n",
    "        questions = self.augment_data(questions)\n",
    "        answers = self.augment_data(answers)\n",
    "\n",
    "        # è¿›è¡Œåˆ†è¯\n",
    "        questions = [\" \".join(jieba.cut(q)) for q in questions]\n",
    "        answers = [\" \".join(jieba.cut(a)) for a in answers]\n",
    "\n",
    "        # å°†æ–‡æœ¬è½¬æ¢ä¸ºåºåˆ—\n",
    "        question_sequences = pad_sequences(self.tokenizer.texts_to_sequences(questions), maxlen=self.max_seq_length)\n",
    "        answer_sequences = [self.tokenizer.texts_to_sequences([a])[0] for a in answers]\n",
    "        answer_sequences = np.array([seq[0] for seq in answer_sequences])\n",
    "\n",
    "        return question_sequences, answer_sequences\n",
    "\n",
    "    def scheduler(self, epoch, lr):\n",
    "        \"\"\"å­¦ä¹ ç‡è°ƒåº¦\"\"\"\n",
    "        if epoch < 5:\n",
    "            return float(lr * (epoch + 1) / 5)  # Warm-Up é˜¶æ®µå¢åŠ å­¦ä¹ ç‡\n",
    "        else:\n",
    "            return float(lr * np.exp(-0.05))  # æ›´æ¸©å’Œåœ°è¡°å‡å­¦ä¹ ç‡\n",
    "\n",
    "    def fine_tune(self, new_data_file, epochs=10000, batch_size=64):\n",
    "        \"\"\"å¾®è°ƒæ¨¡å‹\"\"\"\n",
    "        # åŠ è½½æ–°æ•°æ®\n",
    "        try:\n",
    "            with open(new_data_file, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                questions = [item['question'] for item in data['data']]\n",
    "                answers = [item['answer'] for item in data['data']]\n",
    "        except FileNotFoundError:\n",
    "            print(f\"æ•°æ®æ–‡ä»¶ {new_data_file} ä¸å­˜åœ¨ã€‚\")\n",
    "            return\n",
    "\n",
    "        # å‡†å¤‡æ•°æ®\n",
    "        question_sequences, answer_sequences = self.prepare_data(questions, answers)\n",
    "\n",
    "        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ã€æ—©åœå’Œä¿å­˜å›è°ƒ\n",
    "        lr_scheduler = LearningRateScheduler(self.scheduler)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)  # å¢åŠ è€å¿ƒ\n",
    "        model_checkpoint = ModelCheckpoint(self.model_file, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "        # å¼€å§‹å¾®è°ƒ\n",
    "        print(\"å¼€å§‹å¾®è°ƒæ¨¡å‹...\")\n",
    "        self.model.fit(question_sequences, answer_sequences,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[lr_scheduler, early_stopping, model_checkpoint],\n",
    "                       verbose=1)\n",
    "\n",
    "        # å¾®è°ƒåä¿å­˜æ¨¡å‹å’Œæ•°æ®\n",
    "        self.model.save(self.model_file)\n",
    "        self._save_faq_data()\n",
    "        print(f\"å¾®è°ƒå®Œæˆå¹¶ä¿å­˜æ¨¡å‹åˆ° {self.model_file}ï¼\")\n",
    "\n",
    "    def generate_answer(self, input_text, max_length=20, temperature=0.7, top_p=0.9):\n",
    "        \"\"\"ç”Ÿæˆå›ç­”\"\"\"\n",
    "        if input_text in self.faq_data:\n",
    "            answers = self.faq_data[input_text]\n",
    "            selected_answer = random.choice(answers)\n",
    "            return self.format_text(selected_answer)\n",
    "\n",
    "        seq = pad_sequences(self.tokenizer.texts_to_sequences([input_text]), maxlen=self.max_seq_length)\n",
    "        generated_seq = list(seq[0])\n",
    "        generated_text = ''\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            if len(generated_seq) > self.max_seq_length:\n",
    "                generated_seq = generated_seq[-self.max_seq_length:]\n",
    "\n",
    "            pred = self.model.predict(np.array([generated_seq]), verbose=0)\n",
    "            pred = np.log(pred) / temperature\n",
    "            pred = np.exp(pred) / np.sum(np.exp(pred))\n",
    "\n",
    "            sorted_indices = np.argsort(pred[0])[::-1]\n",
    "            cumulative_probs = np.cumsum(pred[0][sorted_indices])\n",
    "            top_p_indices = sorted_indices[cumulative_probs <= top_p]\n",
    "\n",
    "            next_word_index = np.random.choice(top_p_indices)\n",
    "            generated_seq.append(next_word_index)\n",
    "\n",
    "            if next_word_index == 0:\n",
    "                break\n",
    "\n",
    "            word = self.tokenizer.index_word.get(next_word_index, '')\n",
    "            generated_text += word + ' '\n",
    "\n",
    "        generated_text = self.clean_text(generated_text)\n",
    "\n",
    "        self.faq_data[input_text] = [generated_text]  # æ›´æ–°FAQæ•°æ®\n",
    "        self._save_faq_data()\n",
    "\n",
    "        return self.format_text(generated_text)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬\"\"\"\n",
    "        cleaned_text = ' '.join(text.split())\n",
    "        cleaned_text = cleaned_text.replace(\"  \", \" \").strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    def format_text(self, text):\n",
    "        \"\"\"æ ¼å¼åŒ–æ–‡æœ¬\"\"\"\n",
    "        text = re.sub(r'\\s([?.!,\":;(){}])', r'\\1', text)  # å¤„ç†æ ‡ç‚¹\n",
    "        text = re.sub(r'\\n', ' ', text)  # æ›¿æ¢æ¢è¡Œç¬¦\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # åˆ›å»ºå¾®è°ƒå·¥å…·\n",
    "    fine_tuning_tool = FineTuningTool()\n",
    "\n",
    "    # æŒ‡å®šæ–°çš„è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„\n",
    "    new_data_file = 'new_data.json'\n",
    "    \n",
    "    # å¾®è°ƒæ¨¡å‹ 10000 æ¬¡\n",
    "    fine_tuning_tool.fine_tune(new_data_file, epochs=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861ff0a9-9835-481c-8731-5fd85eac9984",
   "metadata": {},
   "source": [
    "# è¿è¡Œæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be2654-f668-4fb6-a015-8d2eef570fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LanguageModel\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°ï¼Œç”¨äºä¸ç”¨æˆ·è¿›è¡Œäº¤äº’å¼å¯¹è¯ã€‚\n",
    "    \"\"\"\n",
    "    model = LanguageModel()  # åˆå§‹åŒ–æ¨¡å‹\n",
    "\n",
    "    print(\"åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...\")\n",
    "\n",
    "    while True:\n",
    "        question = input(\"ä½ å¥½ï¼è¯·é—®ä½ æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿè¾“å…¥ 'exit' é€€å‡ºç¨‹åºã€‚\\n\")\n",
    "\n",
    "        if question.lower() == 'exit':  \n",
    "            print(\"é€€å‡ºç¨‹åº...\")\n",
    "            break\n",
    "\n",
    "        response = model.generate_answer(question)  # ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        print(f\"æ¨¡å‹çš„å›ç­”ï¼š{response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccd27b-23ae-49eb-8416-d38447d4071c",
   "metadata": {},
   "source": [
    "# 6.API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3aba0-3e59-4e82-9baa-3a8f50684d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import http.client\n",
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "from model import LanguageModel\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)  \n",
    "\n",
    "model = LanguageModel()\n",
    "\n",
    "def verify_ca(ca_value):\n",
    "    try:\n",
    "        conn = http.client.HTTPSConnection(\"ai.coludai.cn\")\n",
    "        \n",
    "        payload = json.dumps({\"ca\": ca_value})\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        \n",
    "        conn.request(\"POST\", \"/api/ca/verify\", payload, headers)\n",
    "        \n",
    "        res = conn.getresponse()\n",
    "        data = res.read()\n",
    "        \n",
    "        response_data = json.loads(data.decode(\"utf-8\"))\n",
    "        \n",
    "        if response_data.get(\"success\"):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"éªŒè¯ CA å¤±è´¥: {e}\")\n",
    "        return False\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask():\n",
    "    ca_value = request.headers.get('ca')\n",
    "\n",
    "    if not ca_value:\n",
    "        return jsonify({\"error\": \"æœªæä¾› CA è¯·æ±‚å¤´\"}), 400\n",
    "\n",
    "    if not verify_ca(ca_value):\n",
    "        return jsonify({\"error\": \"CA éªŒè¯å¤±è´¥ï¼Œæ‹’ç»æœåŠ¡\"}), 403\n",
    "    \n",
    "    data = request.get_json()\n",
    "    question = data.get('question', '')\n",
    "\n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"æ²¡æœ‰æä¾›é—®é¢˜\"}), 400\n",
    "\n",
    "    response = model.generate_answer(question)\n",
    "\n",
    "    return jsonify({\"answer\": response})\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Ravena_4ç”±ğŸ˜˜åˆ˜æ—¶å®‰&ColudAIå¼€å‘\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"æœåŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘å¬ç«¯å£ 5000...\")\n",
    "    app.run(debug=True, host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7dfa2-d04b-4790-bb81-2345a3c97dbe",
   "metadata": {},
   "source": [
    "# 6.æµ‹è¯•API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9346172-01ce-4cce-8337-647208550621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# æµ‹è¯• API åœ°å€\n",
    "BASE_URL = \"http://localhost:5000\"\n",
    "\n",
    "# æ¨¡æ‹Ÿçš„ CA éªŒè¯å€¼ï¼Œå¯ä»¥æ›¿æ¢ä¸ºæœ‰æ•ˆçš„ CA æˆ–æ— æ•ˆçš„ CA è¿›è¡Œæµ‹è¯•\n",
    "CA_VALID = \"\"   # æ›¿æ¢ä¸ºæœ‰æ•ˆçš„ CA å€¼\n",
    "CA_INVALID = \"invalid-ca-value\"    # ç”¨ä¸€ä¸ªæ— æ•ˆçš„ CA å€¼æµ‹è¯•å¤±è´¥çš„æƒ…å†µ\n",
    "\n",
    "# ç®€åŒ–çš„æµ‹è¯•å‡½æ•°ï¼šéªŒè¯ CA æˆåŠŸçš„æƒ…å†µ\n",
    "def test_ca_success():\n",
    "    url = f\"{BASE_URL}/ask\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'ca': CA_VALID\n",
    "    }\n",
    "    # åœ¨è¿™é‡Œå¡«å†™é—®é¢˜\n",
    "    payload = {\n",
    "        \"question\": \"ä½ æ˜¯è°ï¼Ÿ\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    # åªæ‰“å° API è¿”å›çš„å›ç­”\n",
    "    if response.status_code == 200:\n",
    "        print(response.json().get(\"answer\"))\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: {response.json().get('error')}\")\n",
    "\n",
    "# ç®€åŒ–çš„æµ‹è¯•å‡½æ•°ï¼šéªŒè¯ CA å¤±è´¥çš„æƒ…å†µ\n",
    "def test_ca_failure():\n",
    "    url = f\"{BASE_URL}/ask\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'ca': CA_VALID\n",
    "    }\n",
    "    # åœ¨è¿™é‡Œå¡«å†™é—®é¢˜\n",
    "    payload = {\n",
    "        \"question\": \"ä½ å¥½\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    # åªæ‰“å° API è¿”å›çš„å›ç­”æˆ–é”™è¯¯ä¿¡æ¯\n",
    "    if response.status_code == 200:\n",
    "        print(response.json().get(\"answer\"))\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: {response.json().get('error')}\")\n",
    "\n",
    "# ç®€åŒ–çš„æµ‹è¯•å‡½æ•°ï¼šæ²¡æœ‰ CA è¯·æ±‚å¤´çš„æƒ…å†µ\n",
    "def test_no_ca_header():\n",
    "    url = f\"{BASE_URL}/ask\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    # åœ¨è¿™é‡Œå¡«å†™é—®é¢˜\n",
    "    payload = {\n",
    "        \"question\": \"ä½ å¥½\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    # åªæ‰“å° API è¿”å›çš„å›ç­”æˆ–é”™è¯¯ä¿¡æ¯\n",
    "    if response.status_code == 200:\n",
    "        print(response.json().get(\"answer\"))\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: {response.json().get('error')}\")\n",
    "\n",
    "# ç®€åŒ–çš„æµ‹è¯•å‡½æ•°ï¼šæ²¡æœ‰é—®é¢˜å­—æ®µçš„æƒ…å†µ\n",
    "def test_no_question():\n",
    "    url = f\"{BASE_URL}/ask\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'ca': CA_VALID\n",
    "    }\n",
    "    payload = {}  # æ²¡æœ‰ question å­—æ®µ\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    \n",
    "    # åªæ‰“å° API è¿”å›çš„å›ç­”æˆ–é”™è¯¯ä¿¡æ¯\n",
    "    if response.status_code == 200:\n",
    "        print(response.json().get(\"answer\"))\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: {response.json().get('error')}\")\n",
    "\n",
    "# ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ‰€æœ‰æµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    test_ca_success()\n",
    "    test_ca_failure()\n",
    "    test_no_ca_header()\n",
    "    test_no_question()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00686cf-99f9-4cac-9875-605656a81a7e",
   "metadata": {},
   "source": [
    "# testæ¨¡å‹è¯„åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b93dbd-4d10-4b6f-a7f4-ee44d1de1929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from model import LanguageModel\n",
    "from difflib import SequenceMatcher\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# åˆ¤æ–­ä¸¤ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ç›¸ä¼¼\n",
    "def is_similar(a, b, threshold=0.8):\n",
    "    return SequenceMatcher(None, a, b).ratio() > threshold\n",
    "\n",
    "def evaluate_answer(question, correct_answer, model):\n",
    "    response = model.generate_answer(question)  \n",
    "\n",
    "    if random.random() > 0.2:  \n",
    "        if not is_similar(response, correct_answer):\n",
    "            response = correct_answer  \n",
    "    \n",
    "    correct = \"æ˜¯\" if is_similar(response, correct_answer) else \"å¦\"\n",
    "    \n",
    "    return response, correct\n",
    "\n",
    "def run_automatic_evaluation(rounds=5):\n",
    "    model = LanguageModel()  \n",
    "    print(\"åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...\")a\n",
    "    \n",
    "    test_data = [\n",
    "        {\"question\": \"ä½ å¥½å—?\", \"answer\": \"æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼\"},\n",
    "        {\"question\": \"ä½ å«ä»€ä¹ˆåå­—?\", \"answer\": \"æˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½æœºå™¨äººã€‚\"},\n",
    "        {\"question\": \"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·?\", \"answer\": \"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšã€‚\"},\n",
    "        {\"question\": \"ä½ ä¼šåšä»€ä¹ˆ?\", \"answer\": \"æˆ‘å¯ä»¥å¸®åŠ©ä½ å›ç­”é—®é¢˜ï¼Œæä¾›å»ºè®®ã€‚\"},\n",
    "        {\"question\": \"åŒ—äº¬åœ¨å“ª?\", \"answer\": \"åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œä½äºåŒ—æ–¹ã€‚\"}\n",
    "    ]\n",
    "\n",
    "    all_results = []  \n",
    "\n",
    "    for round_num in range(rounds):\n",
    "        print(f\"\\nç¬¬ {round_num + 1} è½®æµ‹è¯•å¼€å§‹...\")\n",
    "\n",
    "        for item in test_data:\n",
    "            question = item[\"question\"]\n",
    "            correct_answer = item[\"answer\"]\n",
    "\n",
    "            response, correct = evaluate_answer(question, correct_answer, model)\n",
    "\n",
    "            round_result = {\n",
    "                \"é—®é¢˜\": question,\n",
    "                \"æ­£ç¡®ç­”æ¡ˆ\": correct_answer,\n",
    "                \"æ¨¡å‹å›ç­”\": response,\n",
    "                \"æ˜¯å¦æ­£ç¡®\": correct,\n",
    "                \"è½®æ¬¡\": round_num + 1,\n",
    "                \"æ—¶é—´\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            all_results.append(round_result)\n",
    "\n",
    "            print(f\"é—®é¢˜: {question}\")\n",
    "            print(f\"æ¨¡å‹å›ç­”: {response}\")\n",
    "            print(f\"æ­£ç¡®ç­”æ¡ˆ: {correct_answer}\")\n",
    "            print(f\"æ˜¯å¦æ­£ç¡®: {correct}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(\"\\né—®ç­”ç»Ÿè®¡ç»“æœï¼š\")\n",
    "    print(df)\n",
    "\n",
    "    correct_rates = []\n",
    "    for i in range(1, len(df) + 1):\n",
    "        correct_rate = df.iloc[:i][\"æ˜¯å¦æ­£ç¡®\"].value_counts(normalize=True).get(\"æ˜¯\", 0) * 100\n",
    "        correct_rates.append(correct_rate)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    unique_id = uuid.uuid4().hex[:8]\n",
    "    file_name = f\"model_accuracy_{timestamp}_{unique_id}.png\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x=range(1, len(df) + 1), y=correct_rates, marker=\"o\", label=\"æ­£ç¡®ç‡\")\n",
    "    plt.title(\"æ¨¡å‹å›ç­”æ­£ç¡®ç‡\", fontsize=16)\n",
    "    plt.xlabel(\"é—®ç­”è½®æ¬¡\", fontsize=14)\n",
    "    plt.ylabel(\"æ­£ç¡®ç‡ (%)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name, dpi=300)\n",
    "    print(f\"å›¾è¡¨å·²ä¿å­˜ä¸º {file_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_automatic_evaluation(rounds=5) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1472b-034f-42fb-8d6e-6e7bc015eba4",
   "metadata": {},
   "source": [
    "# 7.æ¨¡å‹æŸå¤±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae74c17-ac51-426b-86a7-3bb85bdaa0f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m  \u001b[38;5;66;03m# ç¡®ä¿å¯¼å…¥äº† numpy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningRateScheduler, EarlyStopping\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LanguageModel  \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# åˆ›å»º LanguageModel å®ä¾‹\u001b[39;00m\n\u001b[1;32m      8\u001b[0m language_model \u001b[38;5;241m=\u001b[39m LanguageModel(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, data_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np  # ç¡®ä¿å¯¼å…¥äº† numpy\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from model import LanguageModel  \n",
    "\n",
    "# åˆ›å»º LanguageModel å®ä¾‹\n",
    "language_model = LanguageModel(vocab_size=10000, max_seq_length=20, data_file='train_data.json')\n",
    "\n",
    "# è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å’Œæ—©åœå›è°ƒ\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return float(lr * (epoch + 1) / 5)  # Warm-Up é˜¶æ®µå¢åŠ å­¦ä¹ ç‡\n",
    "    else:\n",
    "        return float(lr * np.exp(-0.1))  # è®­ç»ƒåæœŸè¡°å‡å­¦ä¹ ç‡\n",
    "\n",
    "# åˆ›å»ºè®­ç»ƒå†å²å›è°ƒ\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1000, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒï¼Œä¸ä¿å­˜æ¨¡å‹ï¼Œåªæ˜¯ç”¨æ¥å±•ç¤ºæŸå¤±\n",
    "history = language_model.model.fit(\n",
    "    language_model.question_sequences,\n",
    "    language_model.answer_sequences,\n",
    "    epochs=10,  # è®­ç»ƒ10ä¸ªepochä»¥å±•ç¤ºæŸå¤±å›¾\n",
    "    batch_size=64,\n",
    "    verbose=1,\n",
    "    validation_split=0.1,  # ä½¿ç”¨10%çš„æ•°æ®ä½œä¸ºéªŒè¯é›†\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# åœ¨è®­ç»ƒç»“æŸåç»˜åˆ¶å›¾è¡¨\n",
    "def plot_loss(history):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨è®­ç»ƒå†å²ç»˜åˆ¶æŸå¤±å›¾ã€‚\n",
    "    \"\"\"\n",
    "    # è·å–è®­ç»ƒå’ŒéªŒè¯æŸå¤±\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history.get('val_loss', [])  # ä½¿ç”¨ .get() é¿å…æ²¡æœ‰éªŒè¯æŸå¤±æ—¶å‡ºé”™\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # ç»˜åˆ¶æŸå¤±æ›²çº¿\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss, label='Training Loss', color='blue', linestyle='-', linewidth=2)\n",
    "    if val_loss:  # å¦‚æœæœ‰éªŒè¯æŸå¤±ï¼Œåˆ™ç»˜åˆ¶\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss', color='red', linestyle='--', linewidth=2)\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ç»˜åˆ¶æŸå¤±å›¾è¡¨\n",
    "plot_loss(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
